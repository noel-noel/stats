[{"id":0,"href":"/stats/hypothesis-testing/non-parametric/chi-squared/","title":"Критерий хи-квадрат","section":"Непараметрические критерии","content":" Критерий хи-квадрат # "},{"id":1,"href":"/stats/interval-estimation/asymptotic-conf-intervals/","title":"Асимптотические доверительные интервалы","section":"Доверительное оценивание","content":" Асимптотические доверительные интервалы # "},{"id":2,"href":"/stats/hypothesis-testing/non-parametric/ks-and-cramer-von-mises-tests/","title":"Критерии Колмогорова-Смирнова и Крамера-фон-Мизеса","section":"Непараметрические критерии","content":" Критерии Колмогорова-Смирнова и Крамера-фон-Мизеса # "},{"id":3,"href":"/stats/point-estimation/methods/mom/","title":"Метод моментов","section":"Методы построения статистических оценок","content":" Метод моментов # "},{"id":4,"href":"/stats/point-estimation/methods/","title":"Методы построения статистических оценок","section":"Параметрическое оценивание","content":" Методы построения статистических оценок # "},{"id":5,"href":"/stats/point-estimation/minimax-bayesian-approaches/","title":"Минимаксный и Байесовский подходы","section":"Параметрическое оценивание","content":" Минимаксный и Байесовский подходы # "},{"id":6,"href":"/stats/point-estimation/methods/mle/","title":"Оценки максимального правдоподобия","section":"Методы построения статистических оценок","content":" Оценки максимального правдоподобия # "},{"id":7,"href":"/stats/point-estimation/problem/","title":"Постановка задачи точечного оценивания","section":"Параметрическое оценивание","content":" Постановка задачи точечного оценивания # "},{"id":8,"href":"/stats/point-estimation/r-efficient-stats/","title":"R-эффективные оценки. Информационное неравенство","section":"Параметрическое оценивание","content":" R-эффективные оценки. Информационное неравенство. # "},{"id":9,"href":"/stats/regression/contingency-tables/","title":"Анализ таблиц сопряженности","section":"Регрессия","content":" Анализ таблиц сопряженности # "},{"id":10,"href":"/stats/non-parametric-estimation/sample-quantiles-asymptotics/","title":"Асимптотическая нормальность выборочных квантилей","section":"Непараметрическое оценивание","content":" Асимптотическая нормальность выборочных квантилей # "},{"id":11,"href":"/stats/point-estimation/mle-asymptotics/","title":"Асимптотические свойства ОМП","section":"Параметрическое оценивание","content":" Асимптотические свойства ОМП # "},{"id":12,"href":"/stats/non-parametric-estimation/probability-distributions/","title":"Вероятностные распределения","section":"Непараметрическое оценивание","content":" Вероятностные распределения # "},{"id":13,"href":"/stats/non-parametric-estimation/sample-methods/","title":"Выборочные подход","section":"Непараметрическое оценивание","content":" Выборочный подход # "},{"id":14,"href":"/stats/non-parametric-estimation/sample-numerical-chars/","title":"Выборочные числовые характеристики","section":"Непараметрическое оценивание","content":" Выборочные числовые характеристики # "},{"id":15,"href":"/stats/regression/anova/two-way-anova/","title":"Двухфакторный дисперсионный анализ","section":"Дисперсионный анализ","content":" Двухфакторный дисперсионный анализ # "},{"id":16,"href":"/stats/regression/gauss-markov/","title":"ДНО-функции. Теорема Гаусса-Маркова.","section":"Регрессия","content":" ДНО-функции. Теорема Гаусса-Маркова. # "},{"id":17,"href":"/stats/interval-estimation/","title":"Доверительное оценивание","section":"О курсе","content":" Доверительное оценивание # "},{"id":18,"href":"/stats/regression/confidence-estimation/","title":"Доверительное оценивание","section":"Регрессия","content":" Доверительное оценивание # "},{"id":19,"href":"/stats/hypothesis-testing/lrt/","title":"Критерий отношения правдоподобий","section":"Проверка гипотез","content":" Критерий отношения правдоподобий # "},{"id":20,"href":"/stats/non-parametric-estimation/fisher-lemma/","title":"Лемма Фишера","section":"Непараметрическое оценивание","content":" Лемма Фишера # "},{"id":21,"href":"/stats/regression/anova/multi-way-anova/","title":"Многофакторный дисперсионный анализ","section":"Дисперсионный анализ","content":" Многофакторный дисперсионный анализ # "},{"id":22,"href":"/stats/hypothesis-testing/ump-for-one-sided-hyp/","title":"Наиболее мощный критерий для проверки односторонней гипотезы","section":"Проверка гипотез","content":" Наиболее мощный критерий для проверки односторонней гипотезы # "},{"id":23,"href":"/stats/hypothesis-testing/non-parametric/","title":"Непараметрические критерии","section":"Проверка гипотез","content":" Непараметрические критерии # "},{"id":24,"href":"/stats/non-parametric-estimation/","title":"Непараметрическое оценивание","section":"О курсе","content":" Непараметрическое оценивание # "},{"id":25,"href":"/stats/point-estimation/unbiased-estimation/","title":"Несмещенное оценивание","section":"Параметрическое оценивание","content":" Несмещенное оценивание # "},{"id":26,"href":"/stats/regression/glm/","title":"Обобщенные линейные модели","section":"Регрессия","content":" Обобщенные линейные модели # "},{"id":27,"href":"/stats/regression/anova/one-way-anova/","title":"Однофакторный дисперсионный анализ","section":"Дисперсионный анализ","content":" Однофакторный дисперсионный анализ # "},{"id":28,"href":"/stats/regression/variance-estimation/","title":"Оценивание мешающего параметра","section":"Регрессия","content":" Оценивание мешающего параметра # "},{"id":29,"href":"/stats/point-estimation/","title":"Параметрическое оценивание","section":"О курсе","content":" Параметрическое оценивание # "},{"id":30,"href":"/stats/point-estimation/ancillary-sufficient-statistics/","title":"Подчиненные и достаточные статистики","section":"Параметрическое оценивание","content":" Подчиненные и достаточные статистики # "},{"id":31,"href":"/stats/regression/examples/","title":"Примеры регрессионных моделей","section":"Регрессия","content":" Примеры регрессионных моделей # "},{"id":32,"href":"/stats/hypothesis-testing/","title":"Проверка гипотез","section":"О курсе","content":" Проверка гипотез # "},{"id":33,"href":"/stats/regression/hypothesis-testing/","title":"Проверка гипотез в модели лин. регрессии","section":"Регрессия","content":" Проверка гипотез в модели лин. регрессии # "},{"id":34,"href":"/stats/hypothesis-testing/ump/","title":"Проверка простой гипотезы при простой альтернативе","section":"Проверка гипотез","content":" Проверка простой гипотезы при простой альтернативе # "},{"id":35,"href":"/stats/regression/","title":"Регрессия","section":"О курсе","content":" Регрессия # "},{"id":36,"href":"/stats/point-estimation/regular-experiment-rao-cramer-inequality/","title":"Регулярный экперимент. Неравенство Рао-Крамера","section":"Параметрическое оценивание","content":" Регулярный экперимент. Неравенство Рао-Крамера. # "},{"id":37,"href":"/stats/introduction/experiment/","title":"Статистический эксперимент","section":"Введение","content":" Статистический эксперимент # Во вводной главе будут ОЧЕНЬ поверхностно обрисованы все темы, которые мы покроем за семестр. Неудивительно, что сходу во все Вы въехать не сможете, останутся вопросы. Это нормально. Мы всё разберем на мелкие детали, Вы ко всему привыкнете, но всему свое время, иначе эта лекция будет километровой.\nМой совет:\nКровь из носу надо въехать в идею параметризации. Во всем, что касается статистик, задач, асимптотики \u0026ndash; следите за логической цепочкой, большего не надо. С чем мы работаем? # Определение\nСтатистическим экспериментом будем называть тройку \\((\\mathfrak{X}, \\mathfrak{F}, \\mathcal{P}=\\{\\mathbb{P}_\\theta: \\theta \\in \\Theta\\})\\), где\n\\(\\mathfrak{X}\\) \u0026ndash; множество исходов\n\\(\\mathfrak{F}\\) \u0026ndash; множество наблюдаемых событий (\\(\\sigma\\)-алгебра на \\(\\mathfrak{X}\\))\n\\(\\mathcal{P}\\) \u0026ndash; семейство распределений\n\\(\\mathbb{P}_\\theta\\) \u0026ndash; конкретное распределение для фиксированного \\(\\theta\\)\n\\(\\Theta\\) \u0026ndash; множество параметра Пояснения Спокойствие! Только спокойствие!\nПо-порядку:\n\\(\\mathfrak{X}\\) \u0026ndash; множество исходов. Здесь всё просто \u0026ndash; множество всего того, что может произойти в нашем эксперименте. Как правило, мы работаем с векторами из \\(\\mathbb{R}^n\\) и подразумеваем, что каждая компонента \u0026ndash; случайная величина, действующая из какого-то абстрактного множества, которое нам неважно. Даже если результаты эксперимента имеют ограниченную область значений, \\(\\mathfrak{X}\\) не сужают до этого множества. Можно просто повесить на ненужные нам области значений нулевую вероятность. \\(\\mathfrak{F}\\) \u0026ndash; множество наблюдаемых событий. В основе статистики лежат вероятностные методы, поэтому нам как и прежде нужно уметь задавать вопросы вероятностного характера, поэтому из теорвера мы заимствуем концепцию \\(\\sigma\\)-алгебры событий на \\(\\mathfrak{X}\\).\nС оставшимся будет долго и нудно, но один раз это увидеть нужно\nДавайте для начала попытаемся наскрести остатки теорвера.\nОпределение\n\\((\\Omega, \\mathfrak{F}, P)\\) \u0026ndash; вероятностный эксперимент, если\n\\(\\mathfrak{F}\\) \u0026ndash; \\(\\sigma\\)-алгебра на \\(\\Omega\\) \\(P: \\mathfrak{F}\\to [0,1]\\) \u0026ndash; вероятностная мера на \\(\\mathfrak{F}\\) В теории вероятностей мы занимались моделированием одного и только одного эксперимента. Мы не задавались вопросом \u0026ldquo;а почему такое распределение?\u0026rdquo;. Мы использовали его как данность. С помощью распределения мы получали вероятностные утверждения об исходах эксперимента. Сейчас наоборот. Имея наблюдения, мы будем пытаться сделать какие-то выводы о распределении.\nНу, не знаю, рост хочу исследовать. Будем брать \\(n\\) человек и измерять рост. Берем по умолчанию \\(\\mathfrak{X} = \\mathbb{R}^n\\), \\(\\mathfrak{F}=\\mathfrak{B}_n\\) (борелевская сигма-алгебра). Даже если очень хочется делать это, как-то опираясь на вероятность, то у нас уже затык. Ибо это все, конечно, очень здорово, но я лично понятия не имею, какое распределение имеет рост человека. В теорвере все было легко и просто (конечно, я утрирую), потому что кто-то дал нам задачу и сказал \u0026ldquo;Случайная величина имеет нормальное распределение\u0026rdquo;. В жизни как-то все сложнее\u0026hellip;\nЧто делать с распределением? Математики додумались дать ответ: \u0026ldquo;Ну раз точно мы ничего не знаем, давайте хотя бы зададим допустимое множество, каким наше распределение может быть\u0026rdquo;. Очень часто мы будем предполагать, что наши показания независимы, поэтому далее речь будет идти об одномерных распределениях.\nВ определенных ситуациях бывает разумно предполагать, что распределение принадлежит какому-то хорошему семейству распределений, например, нормальному. Такое предположение может быть разумно, когда наши наблюдения являются усреднением каких-то ненаблюдаемых нами явно величин. Например, прибор может выдавать не мгновенные значения силы тока, а усредненные в нескольких временных точках показания. Из-за прелести ЦПТ наше предположение о нормальности будет очень даже обоснованным! Вспомним плотность нормального распределения: \\[p(x;a,\\sigma^2)= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-a)^2}{2\\sigma^2}\\right).\\] При конкретных значениях \\((a, \\sigma^2)\\) мы имеем вполне конкретное распределение. Среднее у нас может быть любым, а дисперсия всегда неотрицательна, т.е. если варьировать \\((a, \\sigma^2)\\) по множеству \\(\\Theta=\\mathbb{R}\\times[0, \\infty]\\), то можно получить все нормальные распределения. Итак, параметр \u0026ndash; \\((a, \\sigma^2)\\), параметрическое множество \u0026ndash; \\(\\Theta=\\mathbb{R}\\times[0, \\infty]\\), \\(\\mathbb{P}_{\\theta_0=(a_0,\\sigma_0^2)}\\), где \\(\\theta_0 \\in \\Theta\\) \u0026ndash; конкретное нормальное распределение, имеющее плотность \\(p(x;a_0,\\sigma_0^2)\\). А наше \\(\\mathcal{P}\\) \u0026ndash; все возможные нормальные распределения, которые можно получить варьированием \\(\\theta=(a, \\sigma^2)\\) по \\(\\Theta\\), т.e. \\(\\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta\\}\\). Изучением свойств экспериментов с конечномерными параметрическими множествами занимается параметрическая статистика. В худшем случае, когда мы вообще ничего не знаем (или не хотим думать), то прямо и говорим \u0026ndash; \u0026ldquo;хз, может быть что угодно\u0026rdquo;. Ну, я человек простой, беру и все возможные распределения помещаю в \\(\\mathcal{P}\\). Распределение (см. определение) \u0026ndash; штука сложная, поэтому статистики вполне естественно характеризуют его чем-то проще, например, функцией распределения (\\(\\theta=F\\)), в таком случае \\(\\Theta\\) \u0026ndash; множество всех-всех функций распределений, а \\(\\mathcal{P}\\) \u0026ndash; множество всех-всех распределений, которые можно получить, если перебрать все-все функции распределения. Удобно ли? Безусловно! Насколько содержательные выводы можно получить, если совсем ничего не предполагать? Узнаем в следующих сериях о непараметрической статистике\u0026hellip; \u0026ldquo;Статистика не сахар\u0026rdquo; С.В.\nЗадачи мат. статистики # Реализация эксперимента дает некоторое \\(X\\in \\mathfrak{X}\\). Задача статистики \u0026ndash; сделать выводы об истинном значении \\(\\theta \\in \\Theta\\). Комментарий Это очень-мега-супер важно!!! По сути, мы весь курс долбаемся с тем, как бы нам что-то узнать о \\(\\theta\\). Истинное значение \\(\\theta\\) мы не знаем и никогда не узнаем, но мы очень хотим к нему приблизиться. По мере изучения курса Вы почувствуете, почему нам это важно и как с этим работать. Какие выводы? Для ответа на этот вопрос перечислим типы задач математической статистики. Каждый тип мы разберем подробнее чуть позже, сейчас просто ознакомимся с тем, что можно делать.\nИтак, в статистике можно заниматься\nТочечным оцениванием:\nКак прикинуть неизвестное нам теоретическое значение \\(\\theta\\)?\nОтвет задачи: наилучшая прикидка для параметра. Доверительным оцениванием:\nКак прикинуть \\(\\theta\\) с уверенностью \\(1-\\alpha\\) (напр. с вероятностью 0.95 мой параметр лежит от a до b)?\nОтвет задачи \u0026ndash; как правило, интервал, границы которого зависят от наблюдений \\([T_1(X), T_2(X)]: P([T_1(X), T_2(X)] \\ni \\theta) = 1-\\alpha\\) Проверкой гипотез:\nКак задать вопросы к данным? О чем вообще можно спросить? Похожи ли наши данные на распределение-нейм, зависят ли наши данные от параметр-нейм (напр. возраст), похожи ли два набора данных друг на друга \u0026ndash; на такие человеческие вопросы мы научимся отвечать с точки зрения статистики. и еще много чем\u0026hellip; Все задачи решаются с использованием понятия статистики.\nОпределение\nСтатистикой будем называть измеримую функцию \\(T:\\mathfrak{X}\\to\\mathbb{R}^n\\). Пояснения Да, статистика в статистике, и что Вы мне сделаете???\nНу, а если серьезно, то\u0026hellip; Забудьте на секунду про весь теорвер, то, что Вы только что прочитали про стат. эксперимент. Попытайтесь вспомнить, все те моменты, когда Вы слышали\\видели слово \u0026ldquo;статистика\u0026rdquo;. Статистика игры (дота), статистика матчей (футбол), статистика приемной кампании\u0026hellip; Обычно за этим следуют всякие разные числа\u0026hellip; В общем-то мат. статистика об этом же (правда, более пафосно)! У Вас есть эксперимент. Результат экперимента \u0026ndash; как правило вектор из \\(\\mathbb{R}^n\\). Ну, получили Вы вектор длины, допустим, 1000 (показания с 1000 человек). Не будем же мы глазами на него смотреть \u0026ndash; какой с этого толк. Нас обычно интересует информация попроще и покороче. Ну не знаю, среднее там, разброс, количество экстремальных значений, всякие разные мудреные коэффициенты\u0026hellip; Вот это все и есть статистики.\nТо, что должно остаться в голове:\nСтатистика \u0026ndash; функция от наблюдений.\nКакая? Да любая, какая хотите (все, что придёт в голову психически здоровому человеку \u0026ndash; измеримо). Даже тупая функция от наблюдений \u0026ndash; тоже статистика. Например, мне 5 лет, и я буду каждому наблюдению сопоставлять число 1, потому что другого я не знаю.\nНапример, функция f \u0026ndash; статистика:\nimport numpy as np def f(x): return np.mean(x), np.min(x), np.min(x) Модель # Коротко, о раннее уже сказанном:\n\\(\\mathfrak{X}\\) определяется видом получаемых данных, как правило, \\(\\mathfrak{X}=\\mathbb{R}^n\\).\n\\(\\mathfrak{F}\\) вообще говоря тоже зависит от вида данных. но как правило не думают и берут \\(\\mathfrak{B}_n\\).\n\\(\\mathcal{P}=\\{P_\\theta:\\theta\\in\\Theta\\}\\) \u0026ndash; параметризация эксперимента, где \\(P_\\theta\\) \u0026ndash; обычно распределение случайного вектора.\nВ зависимости от вида \\(\\Theta\\) различают задачи:\nПараметрические (\\(\\dim\\Theta = k\\)) Семипараметрические (\\(\\Theta=\\Theta_1\\times\\Theta_2: \\dim \\Theta_1=k,\\dim\\Theta_2=\\infty\\)) Непараметрические (все остальные) P.S. Про семипараметрические модели мы не успеем поговорить, а привести простенький пример не так-то просто, поэтому, пожалуйста, поверьте нам, что между параметрикой и непараметрикой есть что-то еще :).\nДалее будем считать, что наши данные имеют вид \\(\\Xbi = (X_1,\\dots, X_n) \\in \\mathfrak{X}\\), также будем называть \\(X_i\\) наблюдениями.\nСхемы накопления информации # Выборка # Определение\nВ эксперименте с \\((\\mathfrak{X},\\mathfrak{F}) = (\\mathbb{R}^n, \\mathfrak{B}_n)\\) выборкой будем называть \\(X_1,\\dots, X_n\\), если они суть независимые одинаково распределенные случайные величины (НОРСВ). При работе с выборкой мы имеем дело с распределением случайного вектора с независимыми одинаково распределенными компонентами: \\[\\mathbb{P}_\\theta (X_1 \\in A_1,\\dots,X_n \\in A_n) = P_\\theta (A_1)\\times\\dots\\times P_\\theta (A_n),\\] то есть \\[\\mathbb{P}_\\theta = P_\\theta\\times\\dots\\times P_\\theta.\\] Из этого следует, что нет разницы: работать ли с семейством распределений случайного вектора или же с семейством распределений для одной компоненты (компоненты-то все \u0026ldquo;одинаковые\u0026rdquo;). Если мы знаем распределение одной компоненты \u0026ndash; мы знаем все! Очевидно, что между распределением вектора и распределением одной случайной величины мы выберем последнее \u0026ndash; оно ж проще! Итак, шо то, шо это\u0026hellip;: \\[\\{\\mathbb{P}_\\theta: \\theta\\in\\Theta\\} \\leftrightarrow \\{P_\\theta: \\theta\\in\\Theta\\}.\\] Как ставить эксперимент, чтобы впоследствии суметь сделать выводы о \\(\\theta\\)? В случае с одной выборкой рассматривают:\nПовторное проведение эксперимента \\(n\\) раз в одних и тех же условиях. Выбор из генеральной совокупности (статистически однородное множество объектов):\n\\(N\\) \u0026ndash; количество элементов генеральной совокупности,\n\\(n\\) \u0026ndash; количество исследуемых нами объектов (случайно из \\(N\\)).\nСтрого говоря, вспоминая задачи с картами, мы можем сказать, что выбор из совокупности без возвращения влечет зависимость событий, но при \\(n\\: \u0026laquo;\\: N\\) наш набор можно считать выборкой. Несколько выборок # Пусть \\(m\\) \u0026ndash; количество выборок (фиксированное число)\nНаблюдения: \\((X_1, z_1), \\dots, (X_n, z_n)\\), где\n\\(X_i\\) \u0026ndash; интересующая нас характеристика,\n\\(z_i\\in \\{1,\\dots, m\\}\\) \u0026ndash; номер выборки.\nИсследуем \\(P_{\\theta, z} \\) \u0026ndash; распределение наблюдений при фиксированном значении \\(z\\).\nРегрессия # Отметим, что зачастую мы имеем не только сведения об интересующем нас показателе, например росте, но так же некоторый набор сопутствующих характеристик/факторов. Например, вес, пол, этничность и проч. Такие признаки признаки называются ковариатами. Они не являются ключевой целью исследования, однако вполне естественно предполагать, что какие-то из них могут оказывать влияние на результат эксперимента. Как учесть это самое влияние? Ответов вагон и маленькая тележка, мы же остановимся на регрессионных методах.\nИтак, статистические данные имеют вид:\n\\((Y_1, {\\bf z}_1), \\dots,(Y_n, {\\bf z}_n),\\) где\n\\(Y_i\\) \u0026ndash; наблюдение (интересующая нас характеристика),\n\\({\\bf z}_i\\) \u0026ndash; вектор ковариат.\nИзучаем \\(P_{\\theta, {\\bf z}} \\) \u0026ndash; распределение наблюдений при фиксированном значении \\({\\bf z}\\in\\mathbb{R}^d\\).\nОпределение\nРегрессией случайной величины \\(Y\\) по \\(X\\) будем называть \\(\\E(Y|X)\\). В рассматриваемой нами модели имеем: \\(\\E_\\theta(Y|{\\bf z})\\) \u0026ndash; мат. ожидание \\(Y\\) при фиксированном \\({\\bf z}\\).\nОпределение\nРегрессионная модель:\n\\[\\E_\\theta(Y|{\\bf z}) = g({\\bf z};\\theta),\\] где \\(g({\\bf z};\\theta)\\) \u0026ndash; известная, неслучайная функция. Пояснения Моделируем среднее значение какого-то показателя при фиксированных ковариатах. Среднее значение роста человека, если кто-то мне скажет его возраст, вес, этничность и пол. В правой части какая-то ж, ну в смысле \\(g\\). Как среднее зависит от ковариат? ДА ХРЕН ЕГО ЗНАЕТ, ЭТО ЖЕ ПРИРОДНОЕ ЯВЛЕНИЕ. В общем случае в правой части можно писать хоть синус^2(возраста) + 112*вес^3. Да только это как-то мудрено. Не надо усложнять жизнь. По этой и только этой причине повсеместно распространены модели линейной регрессии. Определение\nМодель линейной регрессии:\n\\[\\E_\\theta(Y|{\\bf z}) = \\Xb^T\\!({\\bf z}){\\bf \\beta},\\] \\(\\Xb^T\\!({\\bf z})\\) \u0026ndash; матрица регрессоров,\n\\(\\theta=(\\beta, \\sigma)\\) \u0026ndash; параметр модели Пояснения На всякий случай: мы не можем моделировать (предсказывать) случайную величину. Она же ну\u0026hellip; Случайная\u0026hellip; Поэтому моделируем че попроще \u0026ndash; среднее весьма разумный выбор.\nОчевидно, что правая часть вселяет ужас. По порядку, пусть есть интересующая нас характеристика \\(Y\\), и две ковариаты \\(z_1, z_2\\) (возраст и вес, напр.). Моделирую \\(\\E(Y|z_1,z_2)\\) линейной функцией, например, \\(\\beta_0 + \\beta_1z_1+\\beta_2z_2\\). Ну типа \\(kx+b\\)\u0026hellip; Где беты можно интерпретировать как степень влияния каждой из ковариат. А так, это параметры, КОТОРЫЕ МЫ НЕ ЗНАЕМ И НИКОГДА НЕ УЗНАЕМ, но очень хотим узнать. Как мы их оценим? Потом-потом-потом. Щас главное модель. А можно ли моделировать как \\(\\beta_0 + \\beta_1\\sin(z_1)+\\beta_2z_2^3\\)? Можно. Модель линейна в параметрах.\nДальше вспоминаем умножение матриц\u0026hellip; Пусть есть \\(n\\) человек, мы с них собрали данные и знаем как игреки \\(\\Ybi=(Y_1,\\dots,Y_n)^T\\), так и зет \\((z_{11},z_{12}),\\dots, (z_{n1},z_{n2})\\). Для каждого наблюдения можно написать нашу модель: \\[\\E(Y_1|z_1,z_2) = \\beta_0 + \\beta_1z_{11}+\\beta_2z_{12}\\] \\[\\dots\\] \\[\\E(Y_n|z_1,z_2) = \\beta_0 + \\beta_1z_{n1}+\\beta_2z_{n2}\\] \\[\\E(\\Ybi\\;|{\\bf z}) = \\begin{pmatrix} 1 \u0026amp; z_{11} \u0026amp; z_{12} \\\\ \\dots \u0026amp; \\dots \u0026amp; \\dots \\\\ 1 \u0026amp; z_{n1} \u0026amp; z_{n2} \\\\ \\end{pmatrix} \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2 \\end{pmatrix}=\\Xb^T\\!({\\bf z}){\\bf \\beta} \\] Как уже упоминалось, можно добавить к ковариатам степени и прочую лабуду, поэтому матрица \\({\\bf X}\\) будет как-то зависеть от ковариат. Нахрена транспонирование? Вопрос риторический. Можно найти этому оправдания, но я не перестану верить, что кто-то просто любит выделываться.\nВ определении модели сказано, что еще есть некоторая сигма\u0026hellip; Короткий ответ следующий: одно лишь среднее (что мы моделируем) не характеризует распределение, его недостаточно. Мы будем работать с нормальным распределением, для которого среднего и дисперсии будет достаточно. Не грузитесь сейчас.\nАсимптотический подход # Строго говоря, в подавляющем большинстве случаев статистика, при применении ее к нашим прикладным задачам с конечным размером выборки \\(n\\), не умеет давать нам ответы на задаваемые вопросы. Потому что потому. Потом увидите, что разработано, и поймете, собственно, почему так. В любом случае, по этой причине активно применяются так называемые асимптотические методы.\nРассмотрим последовательность стат. экспериментов \\((\\mathfrak{X}_n, \\mathfrak{F}_n, \\mathcal{P}_n), \\mathcal{P}_n=\\{\\mathbb{P}_{n,\\theta}: \\theta \\in \\Theta\\}\\) при \\(n\\to\\infty\\).\nПояснения Как смотреть на эту прелесть?\nВо-первых, ЛЮБАЯ асимптотика в этой жизни \u0026ndash; всего лишь теория. Во-вторых, вот прямо сейчас, читая это в первый раз, всего прикола вы не почувствуете. По мере углубления в курс при должном прилежании все дойдет.\nТак вот, давайте построим последовательность гипотетических экспериментов по измерению роста у\n1-го человека, \\((\\mathfrak{X}_1=\\mathbb{R})\\) 2-х человек, \\((\\mathfrak{X}_2=\\mathbb{R}\\times\\mathbb{R})\\) \u0026hellip; \\(n\\) человек, \\((\\mathfrak{X}_n=\\mathbb{R}\\times\\dots\\times\\mathbb{R})\\) \u0026hellip; Да, и так для любого натурального \\(n\\). Смекаешь? Возможно ли это в реальности? Нет. Волнует ли это математиков? Тоже нет. Так, для каждого \\(n\\) имеется своя \\(\\sigma\\)-алгебра \\(\\mathfrak{F}_n\\) и, соответственно, свое семейство распределений \\(\\mathcal{P}_n\\), адаптированное под растущую размерность выборки. Асимптотический подход используется как мысленная реализация идеи, что с увеличением данных мы будем иметь больше информации о явлении и лучше сможем оценивать то, что нам хочется.\nВ задачах точечного оценивания # При каждом \\(n\\) рассматриваем также статистику \\(\\hat\\theta_n:\\mathfrak{X}_n\\to\\Theta\\) для решения той или иной задачи. Пусть \\(\\rho(\\theta_1,\\theta_2)\\) \u0026ndash; расстояние на \\(\\Theta\\).\nОпределение\nОценка \\(\\hat\\theta_n\\) \u0026ndash; состоятельная оценка параметра \\(\\theta\\), если \\[\\forall \\varepsilon \u0026gt; 0 \\quad \\P_\\theta(\\rho(\\hat\\theta_n, \\theta) \u0026gt; \\varepsilon) \\to 0, \\text{ при } n\\to\\infty\\] или \\[\\hat\\theta_n\\to\\theta, \\quad \\forall \\theta\\in\\Theta.\\] Пояснения Оценка \u0026ndash; прикидка для чего-то неизвестного. Нам никто не регламентировал, как строить оценку. Я могу быть дурачком и при растущем объеме выборки для каждой реализации использовать функцию \\(\\hat\\theta_n(\\Xbi) \\equiv 1\\). Это оценка? Да. Есть ли от нее толк? Очень сомневаюсь, ибо информацию о выборке мы как-то не задействуем. То есть, у нас есть какое-то скрытое природой значение параметра \\(\\theta\\), мы собираем 10, 100, 10000 наблюдений и всегда говорим, что \\(\\theta=1\\). И так делать валидно. Но не хорошо. Состоятельность \u0026ndash; один из способов сказать \u0026ldquo;что такое хорошо и что такое плохо\u0026rdquo; в статистике. Чисто с человеческой точки зрения: если мы собираем все больше данных, то хорошая оценка должна приближаться к неизвестному теоретическому значению, ведь информации о явлении все больше. Для любого \\(\\theta\\) пишется потому, что истинного значения \\(\\theta\\) мы не знаем, и мы хотим, чтобы для любого \\(\\theta\\), КАКИМ БЫ ОНО НИ БЫЛО, оценка (функция, зависящая от наблюдений) сходилась по вероятности к этому самому \\(\\theta\\).\nВажно\nОпределение ничего не говорит о том, как доказать состоятельность и, к счастью, мы тоже особо не будем этим заниматься, поскольку существуют теоремы для широкого класса оценок, которые мы один раз выведем и после чего скажем, что все хорошо и мы молодцы. Результат теоретический, но на практике несостоятельными оценками не пользуются.\nВ задачах доверительного оценивания # В задачах доверительного оценивания можно говорить об асимптотических доверительных областях и критериях соответственно.\n\\(\\P_\\theta(\\hat\\Theta_n\\ni\\theta)\\to 1-\\alpha,\\: \\forall \\theta\\in\\Theta\\) \u0026ndash; асимптотическая доверительная область,\n\\(\\P_\\theta(\\hat\\Theta_n\\ni\\theta)\\geq 1-\\alpha,\\: \\forall \\theta\\in\\Theta\\) \u0026ndash; точная доверительная область.\n"}]