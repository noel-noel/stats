<!doctype html><html lang=ru-ru dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Статистический эксперимент # Во вводной главе будут ОЧЕНЬ поверхностно обрисованы все темы, которые мы покроем за семестр. Неудивительно, что сходу во все Вы въехать не сможете, останутся вопросы. Это нормально. Мы всё разберем на мелкие детали, Вы ко всему привыкнете, но всему свое время, иначе эта лекция будет километровой.
Мой совет:
Кровь из носу надо въехать в идею параметризации. Во всем, что касается статистик, задач, асимптотики &ndash; следите за логической цепочкой, большего не надо."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="Статистический эксперимент"><meta property="og:description" content="Статистический эксперимент # Во вводной главе будут ОЧЕНЬ поверхностно обрисованы все темы, которые мы покроем за семестр. Неудивительно, что сходу во все Вы въехать не сможете, останутся вопросы. Это нормально. Мы всё разберем на мелкие детали, Вы ко всему привыкнете, но всему свое время, иначе эта лекция будет километровой.
Мой совет:
Кровь из носу надо въехать в идею параметризации. Во всем, что касается статистик, задач, асимптотики &ndash; следите за логической цепочкой, большего не надо."><meta property="og:type" content="article"><meta property="og:url" content="https://noel-noel.github.io/stats/introduction/experiment/"><meta property="article:section" content="introduction"><title>Статистический эксперимент | Математическая статистика</title><link rel=manifest href=/stats/manifest.json><link rel=icon href=/stats/favicon.png type=image/x-icon><link rel=stylesheet href=/stats/book.min.82c5dbd23447cee0b4c2aa3ed08ce0961faa40e1fa370eee4f8c9f02e0d46b5f.css integrity="sha256-gsXb0jRHzuC0wqo+0Izglh+qQOH6Nw7uT4yfAuDUa18=" crossorigin=anonymous><script defer src=/stats/flexsearch.min.js></script>
<script defer src=/stats/en.search.min.7d5747f166829c7f98f11b9b416ff3971f7e309753857119ddb798fb05a15bbd.js integrity="sha256-fVdH8WaCnH+Y8RubQW/zlx9+MJdThXEZ3beY+wWhW70=" crossorigin=anonymous></script><style type=text/css>.markdown h1,.markdown h2,.markdown h3,.markdown h4,.markdown h5,.markdown h6{font-weight:700;padding-bottom:8px;margin-bottom:8px;border-bottom:solid 1px #e1e1e1;color:#44454f}.markdown p{color:#44454f}.markdown summary:focus{outline:none}</style><link rel=stylesheet href=/stats/katex/katex.min.css><script defer src=/stats/katex/katex.min.js></script>
<script src=/stats/katex/def.js></script>
<script defer src=/stats/katex/auto-render.min.js onload=renderMathInElement(document.body,{macros})></script><style type=text/css>.katex{font-size:1.25em}</style></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/stats/><span>Математическая статистика</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><span>Введение</span><ul><li><a href=/stats/introduction/experiment/ class=active>Статистический эксперимент</a></li></ul></li><li class=book-section-flat><a href=/stats/interval-estimation/>Доверительное оценивание</a><ul><li><a href=/stats/interval-estimation/asymptotic-conf-intervals/>Асимптотические доверительные интервалы</a></li></ul></li><li class=book-section-flat><a href=/stats/non-parametric-estimation/>Непараметрическое оценивание</a><ul><li><a href=/stats/non-parametric-estimation/sample-quantiles-asymptotics/>Асимптотическая нормальность выборочных квантилей</a></li><li><a href=/stats/non-parametric-estimation/probability-distributions/>Вероятностные распределения</a></li><li><a href=/stats/non-parametric-estimation/sample-methods/>Выборочные подход</a></li><li><a href=/stats/non-parametric-estimation/sample-numerical-chars/>Выборочные числовые характеристики</a></li><li><a href=/stats/non-parametric-estimation/fisher-lemma/>Лемма Фишера</a></li></ul></li><li class=book-section-flat><a href=/stats/point-estimation/>Параметрическое оценивание</a><ul><li><input type=checkbox id=section-08719f895623e816cdc4f5b9e210801b class=toggle>
<label for=section-08719f895623e816cdc4f5b9e210801b class="flex justify-between"><a href=/stats/point-estimation/methods/>Методы построения статистических оценок</a></label><ul><li><a href=/stats/point-estimation/methods/mom/>Метод моментов</a></li><li><a href=/stats/point-estimation/methods/mle/>Оценки максимального правдоподобия</a></li></ul></li><li><a href=/stats/point-estimation/minimax-bayesian-approaches/>Минимаксный и Байесовский подходы</a></li><li><a href=/stats/point-estimation/problem/>Постановка задачи точечного оценивания</a></li><li><a href=/stats/point-estimation/r-efficient-stats/>R-эффективные оценки. Информационное неравенство</a></li><li><a href=/stats/point-estimation/mle-asymptotics/>Асимптотические свойства ОМП</a></li><li><a href=/stats/point-estimation/unbiased-estimation/>Несмещенное оценивание</a></li><li><a href=/stats/point-estimation/ancillary-sufficient-statistics/>Подчиненные и достаточные статистики</a></li><li><a href=/stats/point-estimation/regular-experiment-rao-cramer-inequality/>Регулярный экперимент. Неравенство Рао-Крамера</a></li></ul></li><li class=book-section-flat><a href=/stats/hypothesis-testing/>Проверка гипотез</a><ul><li><a href=/stats/hypothesis-testing/lrt/>Критерий отношения правдоподобий</a></li><li><a href=/stats/hypothesis-testing/ump-for-one-sided-hyp/>Наиболее мощный критерий для проверки односторонней гипотезы</a></li><li><input type=checkbox id=section-c5fb984f044fc08e59167831af9031c3 class=toggle>
<label for=section-c5fb984f044fc08e59167831af9031c3 class="flex justify-between"><a href=/stats/hypothesis-testing/non-parametric/>Непараметрические критерии</a></label><ul><li><a href=/stats/hypothesis-testing/non-parametric/chi-squared/>Критерий хи-квадрат</a></li><li><a href=/stats/hypothesis-testing/non-parametric/ks-and-cramer-von-mises-tests/>Критерии Колмогорова-Смирнова и Крамера-фон-Мизеса</a></li></ul></li><li><a href=/stats/hypothesis-testing/ump/>Проверка простой гипотезы при простой альтернативе</a></li></ul></li><li class=book-section-flat><a href=/stats/regression/>Регрессия</a><ul><li><a href=/stats/regression/contingency-tables/>Анализ таблиц сопряженности</a></li><li><input type=checkbox id=section-81ed3af22aa97d67e348bd1417f17caf class=toggle>
<label for=section-81ed3af22aa97d67e348bd1417f17caf class="flex justify-between"><a role=button>Дисперсионный анализ</a></label><ul><li><a href=/stats/regression/anova/two-way-anova/>Двухфакторный дисперсионный анализ</a></li><li><a href=/stats/regression/anova/multi-way-anova/>Многофакторный дисперсионный анализ</a></li><li><a href=/stats/regression/anova/one-way-anova/>Однофакторный дисперсионный анализ</a></li></ul></li><li><a href=/stats/regression/gauss-markov/>ДНО-функции. Теорема Гаусса-Маркова.</a></li><li><a href=/stats/regression/confidence-estimation/>Доверительное оценивание</a></li><li><a href=/stats/regression/glm/>Обобщенные линейные модели</a></li><li><a href=/stats/regression/variance-estimation/>Оценивание мешающего параметра</a></li><li><a href=/stats/regression/examples/>Примеры регрессионных моделей</a></li><li><a href=/stats/regression/hypothesis-testing/>Проверка гипотез в модели лин. регрессии</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/stats/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Статистический эксперимент</strong>
<label for=toc-control><img src=/stats/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#с-чем-мы-работаем>С чем мы работаем?</a></li><li><a href=#задачи-мат-статистики>Задачи мат. статистики</a></li><li><a href=#модель>Модель</a></li><li><a href=#схемы-накопления-информации>Схемы накопления информации</a><ul><li><a href=#выборка>Выборка</a></li><li><a href=#несколько-выборок>Несколько выборок</a></li><li><a href=#регрессия>Регрессия</a></li></ul></li><li><a href=#асимптотический-подход>Асимптотический подход</a><ul><li><a href=#в-задачах-точечного-оценивания>В задачах точечного оценивания</a></li><li><a href=#в-задачах-доверительного-оценивания>В задачах доверительного оценивания</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=статистический-эксперимент>Статистический эксперимент
<a class=anchor href=#%d1%81%d1%82%d0%b0%d1%82%d0%b8%d1%81%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9-%d1%8d%d0%ba%d1%81%d0%bf%d0%b5%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d1%82>#</a></h1><blockquote class="book-hint warning"><p>Во вводной главе будут <strong>ОЧЕНЬ</strong> поверхностно обрисованы все темы, которые мы покроем за семестр. Неудивительно, что сходу во все Вы въехать не сможете, останутся вопросы. Это нормально. Мы всё разберем на мелкие детали, Вы ко всему привыкнете, но всему свое время, иначе эта лекция будет километровой.<br>Мой совет:</p><ol><li>Кровь из носу надо въехать в идею параметризации.</li><li>Во всем, что касается статистик, задач, асимптотики &ndash; следите за логической цепочкой, большего не надо.</li></ol></blockquote><h2 id=с-чем-мы-работаем>С чем мы работаем?
<a class=anchor href=#%d1%81-%d1%87%d0%b5%d0%bc-%d0%bc%d1%8b-%d1%80%d0%b0%d0%b1%d0%be%d1%82%d0%b0%d0%b5%d0%bc>#</a></h2><blockquote class="book-hint info"><strong>Определение</strong><br>Статистическим экспериментом будем называть тройку \((\mathfrak{X}, \mathfrak{F}, \mathcal{P}=\{\mathbb{P}_\theta: \theta \in \Theta\})\), где<br>\(\mathfrak{X}\) &ndash; множество исходов<br>\(\mathfrak{F}\) &ndash; множество наблюдаемых событий (\(\sigma\)-алгебра на \(\mathfrak{X}\))<br>\(\mathcal{P}\) &ndash; семейство распределений<br>\(\mathbb{P}_\theta\) &ndash; конкретное распределение для фиксированного \(\theta\)<br>\(\Theta\) &ndash; множество параметра</blockquote><details><summary>Пояснения</summary><div class=markdown-inner><blockquote><p>Спокойствие! Только спокойствие!</p></blockquote><p>По-порядку:<br>\(\mathfrak{X}\) &ndash; множество исходов. Здесь всё просто &ndash; множество всего того, что может произойти в нашем эксперименте. Как правило, мы работаем с векторами из \(\mathbb{R}^n\) и подразумеваем, что каждая компонента &ndash; случайная величина, действующая из какого-то абстрактного множества, которое нам неважно. Даже если результаты эксперимента имеют ограниченную область значений, \(\mathfrak{X}\) не сужают до этого множества. Можно просто повесить на ненужные нам области значений нулевую вероятность.
\(\mathfrak{F}\) &ndash; множество наблюдаемых событий. В основе статистики лежат вероятностные методы, поэтому нам как и прежде нужно уметь задавать вопросы вероятностного характера, поэтому из теорвера мы заимствуем концепцию \(\sigma\)-алгебры событий на \(\mathfrak{X}\).<br><em><strong>С оставшимся будет долго и нудно, но один раз это увидеть нужно</strong></em><br>Давайте для начала попытаемся наскрести остатки теорвера.<br><strong>Определение</strong><br>\((\Omega, \mathfrak{F}, P)\) &ndash; вероятностный эксперимент, если</p><ul><li>\(\mathfrak{F}\) &ndash; \(\sigma\)-алгебра на \(\Omega\)</li><li>\(P: \mathfrak{F}\to [0,1]\) &ndash; вероятностная мера на \(\mathfrak{F}\)</li></ul><p>В теории вероятностей мы занимались моделированием одного и только одного эксперимента. Мы не задавались вопросом &ldquo;а почему такое распределение?&rdquo;. Мы использовали его как данность. С помощью распределения мы получали вероятностные утверждения об исходах эксперимента. Сейчас наоборот. Имея наблюдения, мы будем пытаться сделать какие-то выводы о распределении.</p><p>Ну, не знаю, рост хочу исследовать. Будем брать \(n\) человек и измерять рост. Берем по умолчанию \(\mathfrak{X} = \mathbb{R}^n\), \(\mathfrak{B}_n\) (борелевская сигма-алгебра). Даже если очень хочется делать это, как-то опираясь на вероятность, то у нас уже затык. Ибо это все, конечно, очень здорово, но я лично понятия не имею, какое распределение имеет рост человека. В теорвере все было легко и просто (конечно, я утрирую), потому что кто-то дал нам задачу и сказал &ldquo;Случайная величина имеет нормальное распределение&rdquo;. В жизни как-то все сложнее&mldr;</p><p>Что делать с распределением? Математики додумались дать ответ: &ldquo;Ну раз точно мы ничего не знаем, давайте хотя бы зададим допустимое множество, каким наше распределение может быть&rdquo;. Очень часто мы будем предполагать, что наши показания независимы, поэтому далее речь будет идти об одномерных распределениях.</p><ul><li>В определенных ситуациях бывает разумно предполагать, что распределение принадлежит какому-то хорошему семейству распределений, например, нормальному. Такое предположение может быть разумно, когда наши наблюдения являются усреднением каких-то ненаблюдаемых нами явно величин. Например, прибор может выдавать не мгновенные значения силы тока, а усредненные в нескольких временных точках показания. Из-за прелести ЦПТ наше предположение о нормальности будет очень даже обоснованным!
Вспомним плотность нормального распределения:
\[p(x;a,\sigma^2)= \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-a)^2}{2\sigma^2}\right).\] При конкретных значениях \((a, \sigma^2)\) мы имеем вполне <strong>конкретное</strong> распределение. Среднее у нас может быть любым, а дисперсия всегда неотрицательна, т.е. если варьировать \((a, \sigma^2)\) по множеству \(\Theta=\mathbb{R}\times[0, \infty]\), то можно получить все нормальные распределения. Итак, параметр &ndash; \((a, \sigma^2)\), параметрическое множество &ndash; \(\Theta=\mathbb{R}\times[0, \infty]\), \(\mathbb{P}_{\theta_0=(a_0,\sigma_0^2)}\), где \(\theta_0 \in \Theta\) &ndash; конкретное нормальное распределение, имеющее плотность \(p(x;a_0,\sigma_0^2)\). А наше \(\mathcal{P}\) &ndash; все возможные нормальные распределения, которые можно получить варьированием \(\theta=(a, \sigma^2)\) по \(\Theta\), т.e. \(\mathcal{P} = \{P_\theta: \theta \in \Theta\}\). Изучением свойств экспериментов с конечномерными параметрическими множествами занимается <em><strong>параметрическая статистика</strong></em>.</li><li>В худшем случае, когда мы вообще ничего не знаем (или не хотим думать), то прямо и говорим &ndash; &ldquo;хз, может быть что угодно&rdquo;. Ну, я человек простой, беру и <strong>все</strong> возможные распределения помещаю в \(\mathcal{P}\). Распределение (см. определение) &ndash; штука сложная, поэтому статистики вполне естественно характеризуют его чем-то проще, например, функцией распределения (\(\theta=F\)), в таком случае \(\Theta\) &ndash; множество всех-всех функций распределений, а \(\mathcal{P}\) &ndash; множество всех-всех распределений, которые можно получить, если перебрать все-все функции распределения. Удобно ли? Безусловно! Насколько содержательные выводы можно получить, если <strong>совсем</strong> ничего не предполагать? Узнаем в следующих сериях о <em><strong>непараметрической статистике</strong></em>&mldr;</li></ul></div></details><blockquote><p>&ldquo;Статистика не сахар&rdquo; С.В.</p></blockquote><h2 id=задачи-мат-статистики>Задачи мат. статистики
<a class=anchor href=#%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b8-%d0%bc%d0%b0%d1%82-%d1%81%d1%82%d0%b0%d1%82%d0%b8%d1%81%d1%82%d0%b8%d0%ba%d0%b8>#</a></h2><blockquote class="book-hint info">Реализация эксперимента дает некоторое \(X\in \mathfrak{X}\). Задача статистики &ndash; сделать выводы об истинном значении \(\theta \in \Theta\).</blockquote><details><summary>Комментарий</summary><div class=markdown-inner>Это очень-мега-супер важно!!! По сути, мы весь курс долбаемся с тем, как бы нам что-то узнать о \(\theta\). Истинное значение \(\theta\) мы не знаем и никогда не узнаем, но мы очень хотим к нему приблизиться. По мере изучения курса Вы почувствуете, почему нам это важно и как с этим работать.</div></details><p>Какие выводы? Для ответа на этот вопрос перечислим типы задач математической статистики. Каждый тип мы разберем подробнее чуть позже, сейчас просто ознакомимся с тем, что можно делать.</p><p>Итак, в статистике можно заниматься</p><ul><li><a href=point-estimation/problem><strong>Точечным оцениванием</strong></a>:<br>Как прикинуть неизвестное нам теоретическое значение \(\theta\)?<br>Ответ задачи: наилучшая прикидка для параметра.</li><li><a href=interval-estimation/><strong>Доверительным оцениванием</strong></a>:<br>Как прикинуть \(\theta\) с уверенностью \(1-\alpha\) (напр. с вероятностью 0.95 мой параметр лежит от a до b)?<br>Ответ задачи &ndash; как правило, интервал, границы которого зависят от наблюдений \([T_1(X), T_2(X)]: P([T_1(X), T_2(X)] \ni \theta) = 1-\alpha\)</li><li><a href=hypothesis-testing/><strong>Проверкой гипотез</strong></a>:<br>Как задать вопросы к данным? О чем вообще можно спросить? Похожи ли наши данные на распределение-нейм, зависят ли наши данные от параметр-нейм (напр. возраст), похожи ли два набора данных друг на друга &ndash; на такие человеческие вопросы мы научимся отвечать с точки зрения статистики.</li><li>и еще много чем&mldr;</li></ul><p>Все задачи решаются с использованием понятия <strong>статистики</strong>.</p><blockquote class="book-hint info"><strong>Определение</strong><br>Статистикой будем называть измеримую функцию \(T:\mathfrak{X}\to\mathbb{R}^n\).</blockquote><details><summary>Пояснения</summary><div class=markdown-inner><p><em><strong>Да, статистика в статистике, и что Вы мне сделаете???</strong></em><br>Ну, а если серьезно, то&mldr; Забудьте на секунду про весь теорвер, то, что Вы только что прочитали про стат. эксперимент. Попытайтесь вспомнить, все те моменты, когда Вы слышали\видели слово &ldquo;статистика&rdquo;. Статистика игры (дота), статистика матчей (футбол), статистика приемной кампании&mldr; Обычно за этим следуют всякие разные числа&mldr; В общем-то мат. статистика об этом же (правда, более пафосно)! У Вас есть эксперимент. Результат экперимента &ndash; как правило вектор из \(\mathbb{R}^n\). Ну, получили Вы вектор длины, допустим, 1000 (показания с 1000 человек). Не будем же мы глазами на него смотреть &ndash; какой с этого толк. Нас обычно интересует информация попроще и покороче. Ну не знаю, среднее там, разброс, количество экстремальных значений, всякие разные мудреные коэффициенты&mldr; Вот это все и есть статистики.</p><p>То, что должно остаться в голове:<br><strong>Статистика &ndash; функция от наблюдений.</strong><br>Какая? Да любая, какая хотите (все, что придёт в голову психически здоровому человеку &ndash; измеримо). Даже тупая функция от наблюдений &ndash; тоже статистика. Например, мне 5 лет, и я буду каждому наблюдению сопоставлять число 1, потому что другого я не знаю.</p><p>Например, функция f &ndash; статистика:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(x), np<span style=color:#f92672>.</span>min(x), np<span style=color:#f92672>.</span>min(x)
</span></span></code></pre></div></div></details><h2 id=модель>Модель
<a class=anchor href=#%d0%bc%d0%be%d0%b4%d0%b5%d0%bb%d1%8c>#</a></h2><p>Коротко, о раннее уже сказанном:<br>\(\mathfrak{X}\) определяется видом получаемых данных, как правило, \(\mathfrak{X}=\mathbb{R}^n\).<br>\(\mathfrak{F}\) вообще говоря тоже зависит от вида данных. но как правило не думают и берут \(\mathfrak{B}_n\).<br>\(\mathcal{P}=\{P_\theta:\theta\in\Theta\}\) &ndash; параметризация эксперимента, где \(P_\theta\) &ndash; обычно распределение случайного вектора.</p><p>В зависимости от вида \(\Theta\) различают задачи:</p><ul><li>Параметрические (\(\dim\Theta = k\))</li><li>Семипараметрические (\(\Theta=\Theta_1\times\Theta_2: \dim \Theta_1=k,\dim\Theta_2=\infty\))</li><li>Непараметрические (все остальные)</li></ul><p>P.S. Про семипараметрические модели мы не успеем поговорить, а привести простенький пример не так-то просто, поэтому, пожалуйста, поверьте нам, что между параметрикой и непараметрикой есть что-то еще :).</p><p>Далее будем считать, что наши данные имеют вид \(\Xbi = (X_1,\dots, X_n) \in \mathfrak{X}\), также будем называть \(X_i\) <strong>наблюдениями</strong>.</p><h2 id=схемы-накопления-информации>Схемы накопления информации
<a class=anchor href=#%d1%81%d1%85%d0%b5%d0%bc%d1%8b-%d0%bd%d0%b0%d0%ba%d0%be%d0%bf%d0%bb%d0%b5%d0%bd%d0%b8%d1%8f-%d0%b8%d0%bd%d1%84%d0%be%d1%80%d0%bc%d0%b0%d1%86%d0%b8%d0%b8>#</a></h2><h3 id=выборка>Выборка
<a class=anchor href=#%d0%b2%d1%8b%d0%b1%d0%be%d1%80%d0%ba%d0%b0>#</a></h3><blockquote class="book-hint info"><strong>Определение</strong><br>В эксперименте с \((\mathfrak{X},\mathfrak{F}) = (\mathbb{R}^n, \mathfrak{B}_n)\) <strong>выборкой</strong> будем называть \(X_1,\dots, X_n\), если они суть независимые одинаково распределенные случайные величины (НОРСВ).</blockquote><p>При работе с выборкой мы имеем дело с распределением случайного вектора с независимыми одинаково распределенными компонентами:
\[\mathbb{P}_\theta (X_1 \in A_1,\dots,X_n \in A_n) = P_\theta (A_1)\times\dots\times P_\theta (A_n),\]
то есть
\[\mathbb{P}_\theta = P_\theta\times\dots\times P_\theta.\]
Из этого следует, что нет разницы: работать ли с семейством распределений случайного вектора или же с семейством распределений для одной компоненты (компоненты-то все &ldquo;одинаковые&rdquo;). Если мы знаем распределение одной компоненты &ndash; мы знаем все! Очевидно, что между распределением вектора и распределением одной случайной величины мы выберем последнее &ndash; оно ж проще! Итак, шо то, шо это&mldr;:
\[\{\mathbb{P}_\theta: \theta\in\Theta\} \leftrightarrow \{P_\theta: \theta\in\Theta\}.\]
Как ставить эксперимент, чтобы впоследствии суметь сделать выводы о \(\theta\)? В случае с одной выборкой рассматривают:</p><ol><li>Повторное проведение эксперимента \(n\) раз в одних и тех же условиях.</li><li>Выбор из генеральной совокупности (статистически однородное множество объектов):<br>\(N\) &ndash; количество элементов генеральной совокупности,<br>\(n\) &ndash; количество исследуемых нами объектов (случайно из \(N\)).<br>Строго говоря, вспоминая задачи с картами, мы можем сказать, что выбор из совокупности без возвращения влечет зависимость событий, но при \(n\: &#171;\: N\) наш набор можно считать выборкой.</li></ol><h3 id=несколько-выборок>Несколько выборок
<a class=anchor href=#%d0%bd%d0%b5%d1%81%d0%ba%d0%be%d0%bb%d1%8c%d0%ba%d0%be-%d0%b2%d1%8b%d0%b1%d0%be%d1%80%d0%be%d0%ba>#</a></h3><p>Пусть \(m\) &ndash; количество выборок (фиксированное число)<br>Наблюдения: \((X_1, z_1), \dots, (X_n, z_n)\), где<br>\(X_i\) &ndash; интересующая нас характеристика,<br>\(z_i\in \{1,\dots, m\}\) &ndash; номер выборки.<br>Исследуем \(P_{\theta, z} \) &ndash; распределение наблюдений при фиксированном значении \(z\).</p><h3 id=регрессия>Регрессия
<a class=anchor href=#%d1%80%d0%b5%d0%b3%d1%80%d0%b5%d1%81%d1%81%d0%b8%d1%8f>#</a></h3><p>Отметим, что зачастую мы имеем не только сведения об интересующем нас показателе, например росте, но так же некоторый набор сопутствующих характеристик/факторов. Например, вес, пол, этничность и проч. Такие признаки признаки называются ковариатами. Они не являются ключевой целью исследования, однако вполне естественно предполагать, что какие-то из них могут оказывать влияние на результат эксперимента. Как учесть это самое влияние? Ответов вагон и маленькая тележка, мы же остановимся на регрессионных методах.</p><p>Итак, статистические данные имеют вид:<br>\((Y_1, {\bf z}_1), \dots,(Y_n, {\bf z}_n),\) где<br>\(Y_i\) &ndash; наблюдение (интересующая нас характеристика),<br>\({\bf z}_i\) &ndash; вектор ковариат.<br>Изучаем \(P_{\theta, {\bf z}} \) &ndash; распределение наблюдений при фиксированном значении \({\bf z}\in\mathbb{R}^d\).</p><blockquote class="book-hint info"><strong>Определение</strong><br>Регрессией случайной величины \(Y\) по \(X\) будем называть \(\E(Y|X)\).</blockquote><p>В рассматриваемой нами модели имеем: \(\E_\theta(Y|{\bf z})\) &ndash; мат. ожидание \(Y\) при фиксированном \({\bf z}\).</p><blockquote class="book-hint info"><strong>Определение</strong><br>Регрессионная модель:<br>\[\E_\theta(Y|{\bf z}) = g({\bf z};\theta),\]
где \(g({\bf z};\theta)\) &ndash; известная, неслучайная функция.</blockquote><details><summary>Пояснения</summary><div class=markdown-inner>Моделируем среднее значение какого-то показателя при фиксированных ковариатах. Среднее значение роста человека, если кто-то мне скажет его возраст, вес, этничность и пол. В правой части какая-то ж, ну в смысле \(g\). Как среднее зависит от ковариат? ДА ХРЕН ЕГО ЗНАЕТ, ЭТО ЖЕ ПРИРОДНОЕ ЯВЛЕНИЕ. В общем случае в правой части можно писать хоть синус^2(возраста) + 112*вес^3. Да только это как-то мудрено. Не надо усложнять жизнь. По этой и только этой причине повсеместно распространены модели линейной регрессии.</div></details><blockquote class="book-hint info"><strong>Определение</strong><br>Модель линейной регрессии:<br>\[\E_\theta(Y|{\bf z}) = \Xb^T\!({\bf z}){\bf \beta},\]
\(\Xb^T\!({\bf z})\) &ndash; матрица регрессоров,<br>\(\theta=(\beta, \sigma)\) &ndash; параметр модели</blockquote><details><summary>Пояснения</summary><div class=markdown-inner><p>На всякий случай: мы не можем моделировать (предсказывать) случайную величину. Она же ну&mldr; Случайная&mldr; Поэтому моделируем че попроще &ndash; среднее весьма разумный выбор.</p><p>Очевидно, что правая часть вселяет ужас.
По порядку, пусть есть интересующая нас характеристика \(Y\), и две ковариаты \(z_1, z_2\) (возраст и вес, напр.). Моделирую \(\E(Y|z_1,z_2)\) линейной функцией, например, \(\beta_0 + \beta_1z_1+\beta_2z_2\). Ну типа \(kx+b\)&mldr; Где беты можно интерпретировать как <strong>степень влияния</strong> каждой из ковариат. А так, это параметры, КОТОРЫЕ МЫ НЕ ЗНАЕМ И НИКОГДА НЕ УЗНАЕМ, но очень хотим узнать. Как мы их оценим? Потом-потом-потом. Щас главное модель. А можно ли моделировать как \(\beta_0 + \beta_1\sin(z_1)+\beta_2z_2^3\)? Можно. Модель линейна в параметрах.</p><p>Дальше вспоминаем умножение матриц&mldr; Пусть есть \(n\) человек, мы с них собрали данные и знаем как игреки \(\Ybi=(Y_1,\dots,Y_n)^T\), так и зет \((z_{11},z_{12}),\dots, (z_{n1},z_{n2})\). Для каждого наблюдения можно написать нашу модель:
\[\E(Y_1|z_1,z_2) = \beta_0 + \beta_1z_{11}+\beta_2z_{12}\]
\[\dots\]
\[\E(Y_n|z_1,z_2) = \beta_0 + \beta_1z_{n1}+\beta_2z_{n2}\]
\[\E(\Ybi\;|{\bf z}) =
\begin{pmatrix}
1 & z_{11} & z_{12} \\
\dots & \dots & \dots \\
1 & z_{n1} & z_{n2} \\
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1\\
\beta_2
\end{pmatrix}=\Xb^T\!({\bf z}){\bf \beta}
\]
Как уже упоминалось, можно добавить к ковариатам степени и прочую лабуду, поэтому матрица \({\bf X}\) будет как-то зависеть от ковариат. Нахрена транспонирование? Вопрос риторический. Можно найти этому оправдания, но я не перестану верить, что кто-то просто любит выделываться.</p><p>В определении модели сказано, что еще есть некоторая сигма&mldr; Короткий ответ следующий: одно лишь среднее (что мы моделируем) не характеризует распределение, его недостаточно. Мы будем работать с нормальным распределением, для которого среднего и дисперсии будет достаточно. Не грузитесь сейчас.</p></div></details><h2 id=асимптотический-подход>Асимптотический подход
<a class=anchor href=#%d0%b0%d1%81%d0%b8%d0%bc%d0%bf%d1%82%d0%be%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9-%d0%bf%d0%be%d0%b4%d1%85%d0%be%d0%b4>#</a></h2><p>Строго говоря, в подавляющем большинстве случаев статистика, при применении ее к нашим прикладным задачам с конечным размером выборки \(n\), <strong>не умеет</strong> давать нам ответы на задаваемые вопросы. Потому что потому. Потом увидите, что разработано, и поймете, собственно, почему так. В любом случае, по этой причине активно применяются так называемые <strong>асимптотические методы</strong>.</p><p>Рассмотрим последовательность стат. экспериментов \((\mathfrak{X}_n, \mathfrak{F}_n, \mathcal{P}_n), \mathcal{P}_n=\{\mathbb{P}_{n,\theta}: \theta \in \Theta\}\) при \(n\to\infty\).</p><details><summary>Пояснения</summary><div class=markdown-inner><p><strong>Как смотреть на эту прелесть?</strong><br>Во-первых, ЛЮБАЯ асимптотика в этой жизни &ndash; всего лишь теория.
Во-вторых, вот прямо сейчас, читая это в первый раз, всего прикола вы не почувствуете. По мере углубления в курс при должном прилежании все дойдет.</p><p>Так вот, давайте построим последовательность гипотетических экспериментов по измерению роста у</p><ul><li>1-го человека, \((\mathfrak{X}_1=\mathbb{R})\)</li><li>2-х человек, \((\mathfrak{X}_2=\mathbb{R}\times\mathbb{R})\)</li><li>&mldr;</li><li>\(n\) человек, \((\mathfrak{X}_n=\mathbb{R}\times\dots\times\mathbb{R})\)</li><li>&mldr; Да, и так для любого натурального \(n\). Смекаешь?</li></ul><p>Возможно ли это в реальности? Нет. Волнует ли это математиков? Тоже нет.
Так, для каждого \(n\) имеется своя \(\sigma\)-алгебра \(\mathfrak{F}_n\) и, соответственно, свое семейство распределений \(\mathcal{P}_n\), адаптированное под растущую размерность выборки. Асимптотический подход используется как мысленная реализация идеи, что с увеличением данных мы будем иметь больше информации о явлении и лучше сможем оценивать то, что нам хочется.</p></div></details><h3 id=в-задачах-точечного-оценивания>В задачах точечного оценивания
<a class=anchor href=#%d0%b2-%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b0%d1%85-%d1%82%d0%be%d1%87%d0%b5%d1%87%d0%bd%d0%be%d0%b3%d0%be-%d0%be%d1%86%d0%b5%d0%bd%d0%b8%d0%b2%d0%b0%d0%bd%d0%b8%d1%8f>#</a></h3><p>При каждом \(n\) рассматриваем также статистику \(\hat\theta_n:\mathfrak{X}_n\to\Theta\) для решения той или иной задачи. Пусть \(\rho(\theta_1,\theta_2)\) &ndash; расстояние на \(\Theta\).</p><blockquote class="book-hint info"><strong>Определение</strong><br>Оценка \(\hat\theta_n\) &ndash; <em><strong>состоятельная оценка</strong></em> параметра \(\theta\), если
\[\forall \varepsilon > 0 \quad \P_\theta(\rho(\hat\theta_n, \theta) > \varepsilon) \to 0, \text{ при } n\to\infty\]
или
\[\hat\theta_n\to\theta, \quad \forall \theta\in\Theta.\]</blockquote><details><summary>Пояснения</summary><div class=markdown-inner><p>Оценка &ndash; прикидка для чего-то неизвестного. Нам никто не регламентировал, <strong>как</strong> строить оценку. Я могу быть дурачком и при растущем объеме выборки для каждой реализации использовать функцию \(\hat\theta_n(\Xbi) \equiv 1\). Это оценка? Да. Есть ли от нее толк? Очень сомневаюсь, ибо информацию о выборке мы как-то не задействуем. То есть, у нас есть какое-то скрытое природой значение параметра \(\theta\), мы собираем 10, 100, 10000 наблюдений и всегда говорим, что \(\theta=1\). И так делать валидно. Но не хорошо. <strong>Состоятельность</strong> &ndash; один из способов сказать &ldquo;что такое хорошо и что такое плохо&rdquo; в статистике. Чисто с человеческой точки зрения: если мы собираем все больше данных, то хорошая оценка должна приближаться к неизвестному теоретическому значению, ведь информации о явлении все больше. Для любого \(\theta\) пишется потому, что истинного значения \(\theta\) мы не знаем, и мы хотим, чтобы для любого \(\theta\), КАКИМ БЫ ОНО НИ БЫЛО, оценка (функция, <strong>зависящая от наблюдений</strong>) сходилась по вероятности к этому самому \(\theta\).</p><p><strong>Важно</strong><br>Определение ничего не говорит о том, <strong>как</strong> доказать состоятельность и, к счастью, мы тоже особо не будем этим заниматься, поскольку существуют теоремы для широкого класса оценок, которые мы один раз выведем и после чего скажем, что все хорошо и мы молодцы. Результат теоретический, но на практике несостоятельными оценками не пользуются.</p></div></details><h3 id=в-задачах-доверительного-оценивания>В задачах доверительного оценивания
<a class=anchor href=#%d0%b2-%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b0%d1%85-%d0%b4%d0%be%d0%b2%d0%b5%d1%80%d0%b8%d1%82%d0%b5%d0%bb%d1%8c%d0%bd%d0%be%d0%b3%d0%be-%d0%be%d1%86%d0%b5%d0%bd%d0%b8%d0%b2%d0%b0%d0%bd%d0%b8%d1%8f>#</a></h3><p>В задачах доверительного оценивания можно говорить об асимптотических доверительных областях и критериях соответственно.</p><p>\(\P_\theta(\hat\Theta_n\ni\theta)\to 1-\alpha,\: \forall \theta\in\Theta\) &ndash; асимптотическая доверительная область,<br>\(\P_\theta(\hat\Theta_n\ni\theta)\geq 1-\alpha,\: \forall \theta\in\Theta\) &ndash; точная доверительная область.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><hr><hr><div style=text-align:right><span><b><i>With gratitude to S.V. Malov by A. Protsvetkina, S. Magnitov</i></b></span><br><span><b><i>Saint-Petersburg Electrotechnical University "LETI"</i></b></span></div></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#с-чем-мы-работаем>С чем мы работаем?</a></li><li><a href=#задачи-мат-статистики>Задачи мат. статистики</a></li><li><a href=#модель>Модель</a></li><li><a href=#схемы-накопления-информации>Схемы накопления информации</a><ul><li><a href=#выборка>Выборка</a></li><li><a href=#несколько-выборок>Несколько выборок</a></li><li><a href=#регрессия>Регрессия</a></li></ul></li><li><a href=#асимптотический-подход>Асимптотический подход</a><ul><li><a href=#в-задачах-точечного-оценивания>В задачах точечного оценивания</a></li><li><a href=#в-задачах-доверительного-оценивания>В задачах доверительного оценивания</a></li></ul></li></ul></nav></div></aside></main></body></html>